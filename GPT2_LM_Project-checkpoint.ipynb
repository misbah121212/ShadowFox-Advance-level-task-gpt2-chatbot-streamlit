{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f402ee-fbc0-48a6-af86-f6176114a53b",
   "metadata": {},
   "source": [
    "# üöÄ Language Model Implementation and Analysis ‚Äî GPT-2 Fine-Tuning\n",
    "\n",
    "**Author:** Misba Sikandar  \n",
    "**Project Level:** Advanced  \n",
    "**Topic:** Natural Language Processing (NLP) ‚Äî Language Model Deployment  \n",
    "**Model Chosen:** GPT-2 (by OpenAI)  \n",
    "**Environment:** Python, Jupyter Notebook, Transformers Library  \n",
    "\n",
    "---\n",
    "\n",
    "### üìò Objective\n",
    "To implement and analyze a Language Model (LM) ‚Äî GPT-2 ‚Äî by fine-tuning it on a text dataset, exploring its text generation capabilities, and understanding its performance and limitations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b468bc-1690-4101-9d51-c9cd8dcc3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü™ú Step 1: Install all necessary libraries\n",
    "\n",
    "# --- Install required packages (if not already installed) ---\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install tqdm and ipywidgets\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    install(\"tqdm\")\n",
    "\n",
    "try:\n",
    "    import ipywidgets\n",
    "except ImportError:\n",
    "    install(\"ipywidgets\")\n",
    "\n",
    "# --- Import required libraries ---\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()  # Enable progress bars for pandas\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "print(\"‚úÖ Step 1 complete: Environment ready, tqdm & ipywidgets imported, GPT-2 tokenizer and model ready to load.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ea590-50fb-4c75-95ce-06793cfc7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 2: Import all necessary libraries\n",
    "\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "print(\"All libraries imported successfully ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484da476-9ac1-45a1-b3cf-45d32b7595a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Step 3: Verify that your dataset (data.txt) exists\n",
    "\n",
    "data_path = \"data.txt\"\n",
    "print(os.path.exists(data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9088e54-9803-4216-9057-5c4b6c1c55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Step 4: Load and inspect your dataset\n",
    "\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(text_data[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab9fdd-b593-41e4-9f55-24d1c734cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Step 5: Load GPT-2 tokenizer and model\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Because we added a pad_token to the tokenizer, we must resize the model embeddings\n",
    "# Using mean_resizing=False to disable the info message about new embeddings\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# Ensure the model uses the correct pad token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Tokenizer and model loaded successfully ‚úÖ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3d187-1aa9-4115-b7dd-4fd604ebf3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß± Step 6: Prepare dataset for training\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size=128):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=block_size,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = tokenized.input_ids\n",
    "        self.attn_masks = tokenized.attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attn_masks[idx],\n",
    "            \"labels\": self.input_ids[idx]\n",
    "        }\n",
    "\n",
    "dataset = TextDataset(text_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader ready ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2e921-cc9d-4e44-b51c-f167d131d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fda6e-9986-4467-9718-0344eabb14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Step 7: Fine-tune the model\n",
    "\n",
    "# =====================================================\n",
    "# STEP 7: INITIALIZE TRAINER AND START TRAINING\n",
    "# =====================================================\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# ---------------------------------------\n",
    "# Reload tokenizer and model (so no errors)\n",
    "# ---------------------------------------\n",
    "model_path = \"./models/gpt2-finetuned\"  # Change if your model path differs\n",
    "try:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "except:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data collator and training arguments\n",
    "# ---------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset again\n",
    "dataset = load_dataset('text', data_files={'train': r'C:\\Users\\misba\\OneDrive\\Desktop\\LM_Project\\data.txt'})\n",
    "\n",
    "# Tokenize again\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Prepare data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Start training\n",
    "# ---------------------------------------\n",
    "print(\"üöÄ Training started...\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Training finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e09e9-7aa6-44dd-a686-cdf16eb7ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ STEP 8: Save & Test Your Fine-Tuned Model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1Ô∏è‚É£ Save model and tokenizer\n",
    "save_path = \"./models/gpt2-finetuned\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer saved successfully at:\", save_path)\n",
    "\n",
    "# 2Ô∏è‚É£ Load the fine-tuned model for text generation\n",
    "generator = pipeline(\"text-generation\", model=save_path, tokenizer=save_path, device=-1)  # device=-1 means CPU\n",
    "\n",
    "# 3Ô∏è‚É£ Test your model with a custom prompt\n",
    "prompt = \"Hello, my name is Misba and I am working on\"\n",
    "output = generator(prompt, max_length=80, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "print(\"\\nüß† Generated Text:\\n\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d86893-fbd2-4a7e-8191-0a8aa7951978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# üßæ STEP 9: SAVE AND RELOAD YOUR FINE-TUNED MODEL\n",
    "# =====================================================\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# ‚úÖ Define save directory\n",
    "save_directory = \"./gpt2-finetuned-model\"\n",
    "\n",
    "# ‚úÖ Save model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"‚úÖ Model and tokenizer saved successfully at:\", save_directory)\n",
    "\n",
    "# =====================================================\n",
    "# üîÅ To verify: Reload the model and tokenizer\n",
    "# =====================================================\n",
    "print(\"\\nüîÅ Reloading the model to confirm...\")\n",
    "\n",
    "# Load back from the saved directory\n",
    "reloaded_tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "reloaded_model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "print(\"‚úÖ Reloaded model and tokenizer successfully!\")\n",
    "\n",
    "# =====================================================\n",
    "# üí¨ Quick test: Generate text from the reloaded model\n",
    "# =====================================================\n",
    "\n",
    "prompt = \"Once upon a time in the world of AI,\"\n",
    "inputs = reloaded_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"\\nü§ñ Generating text...\")\n",
    "outputs = reloaded_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n‚ú® Model Output:\\n\")\n",
    "print(reloaded_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
