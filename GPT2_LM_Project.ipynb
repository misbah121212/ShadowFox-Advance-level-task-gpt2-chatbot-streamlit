{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f402ee-fbc0-48a6-af86-f6176114a53b",
   "metadata": {},
   "source": [
    "# 🚀 Language Model Implementation and Analysis — GPT-2 Fine-Tuning\n",
    "\n",
    "**Author:** Misba Sikandar  \n",
    "**Project Level:** Advanced  \n",
    "**Topic:** Natural Language Processing (NLP) — Language Model Deployment  \n",
    "**Model Chosen:** GPT-2 (by OpenAI)  \n",
    "**Environment:** Python, Jupyter Notebook, Transformers Library  \n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Objective\n",
    "To implement and analyze a Language Model (LM) — GPT-2 — by fine-tuning it on a text dataset, exploring its text generation capabilities, and understanding its performance and limitations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b468bc-1690-4101-9d51-c9cd8dcc3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1 complete: Environment ready, tqdm & ipywidgets imported, GPT-2 tokenizer and model ready to load.\n"
     ]
    }
   ],
   "source": [
    "# 🪜 Step 1: Install all necessary libraries\n",
    "\n",
    "# --- Install required packages (if not already installed) ---\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install tqdm and ipywidgets\n",
    "try:\n",
    "    import tqdm\n",
    "except ImportError:\n",
    "    install(\"tqdm\")\n",
    "\n",
    "try:\n",
    "    import ipywidgets\n",
    "except ImportError:\n",
    "    install(\"ipywidgets\")\n",
    "\n",
    "# --- Import required libraries ---\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()  # Enable progress bars for pandas\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "print(\"✅ Step 1 complete: Environment ready, tqdm & ipywidgets imported, GPT-2 tokenizer and model ready to load.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346ea590-50fb-4c75-95ce-06793cfc7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully ✅\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ Step 2: Import all necessary libraries\n",
    "\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "print(\"All libraries imported successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484da476-9ac1-45a1-b3cf-45d32b7595a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 📁 Step 3: Verify that your dataset (data.txt) exists\n",
    "\n",
    "data_path = \"data.txt\"\n",
    "print(os.path.exists(data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9088e54-9803-4216-9057-5c4b6c1c55eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am working on a Language Model project.\n",
      "This project fine-tunes GPT-2 using custom text data.\n",
      "The model learns to generate sentences related to AI and NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Step 4: Load and inspect your dataset\n",
    "\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(text_data[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ab9fdd-b593-41e4-9f55-24d1c734cb18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m model = GPT2LMHeadModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Because we added a pad_token to the tokenizer, we must resize the model embeddings\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Using mean_resizing=False to disable the info message about new embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model.resize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m), mean_resizing=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Ensure the model uses the correct pad token\u001b[39;00m\n\u001b[32m     13\u001b[39m model.config.pad_token_id = tokenizer.pad_token_id\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 🧩 Step 5: Load GPT-2 tokenizer and model\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Because we added a pad_token to the tokenizer, we must resize the model embeddings\n",
    "# Using mean_resizing=False to disable the info message about new embeddings\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "\n",
    "# Ensure the model uses the correct pad token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Tokenizer and model loaded successfully ✅\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3d187-1aa9-4115-b7dd-4fd604ebf3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧱 Step 6: Prepare dataset for training\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size=128):\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=block_size,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = tokenized.input_ids\n",
    "        self.attn_masks = tokenized.attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attn_masks[idx],\n",
    "            \"labels\": self.input_ids[idx]\n",
    "        }\n",
    "\n",
    "dataset = TextDataset(text_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Dataset and DataLoader ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2e921-cc9d-4e44-b51c-f167d131d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fda6e-9986-4467-9718-0344eabb14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Step 7: Fine-tune the model\n",
    "\n",
    "# =====================================================\n",
    "# STEP 7: INITIALIZE TRAINER AND START TRAINING\n",
    "# =====================================================\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# ---------------------------------------\n",
    "# Reload tokenizer and model (so no errors)\n",
    "# ---------------------------------------\n",
    "model_path = \"./models/gpt2-finetuned\"  # Change if your model path differs\n",
    "try:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "except:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data collator and training arguments\n",
    "# ---------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset again\n",
    "dataset = load_dataset('text', data_files={'train': r'C:\\Users\\misba\\OneDrive\\Desktop\\LM_Project\\data.txt'})\n",
    "\n",
    "# Tokenize again\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Prepare data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Start training\n",
    "# ---------------------------------------\n",
    "print(\"🚀 Training started...\")\n",
    "trainer.train()\n",
    "print(\"✅ Training finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e09e9-7aa6-44dd-a686-cdf16eb7ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ STEP 8: Save & Test Your Fine-Tuned Model\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1️⃣ Save model and tokenizer\n",
    "save_path = \"./models/gpt2-finetuned\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ Model and tokenizer saved successfully at:\", save_path)\n",
    "\n",
    "# 2️⃣ Load the fine-tuned model for text generation\n",
    "generator = pipeline(\"text-generation\", model=save_path, tokenizer=save_path, device=-1)  # device=-1 means CPU\n",
    "\n",
    "# 3️⃣ Test your model with a custom prompt\n",
    "prompt = \"Hello, my name is Misba and I am working on\"\n",
    "output = generator(prompt, max_length=80, num_return_sequences=1, temperature=0.7)\n",
    "\n",
    "print(\"\\n🧠 Generated Text:\\n\")\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98b6237-f698-4efe-ab1b-5ff82221bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misba\\OneDrive\\Desktop\\LM_Project\\.venv\\Scripts\\python.exe\n",
      "C:\\Users\\misba\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ee6293-9823-44b8-9378-e37778f291e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer loaded from: ./models/gpt2-finetuned\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model directory (the one you trained earlier)\n",
    "model_path = \"./models/gpt2-finetuned\"  # or the folder where your fine-tuned model was saved\n",
    "\n",
    "# Load model and tokenizer from there\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(\"✅ Model and tokenizer loaded from:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ce9309-1806-49da-adf0-ab3bd7d05ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer loaded from: ./models/gpt2-finetuned\n",
      "✅ Model and tokenizer saved successfully at: ./gpt2-finetuned-model\n",
      "\n",
      "🔁 Reloading the model to confirm...\n",
      "✅ Reloaded model and tokenizer successfully!\n",
      "\n",
      "🤖 Generating text...\n",
      "\n",
      "✨ Model Output:\n",
      "\n",
      "Once upon a time in the world of AI, the world of AI, the world of AI has been completely different. AI is not simply a language, it is also a computer. In the language of AI, the data that is being processed is being processed, and the data that is being processed is being stored. That is, the data can be written by any program, and its data can be read by any program.\n",
      "\n",
      "On the one hand, this is true of a language such as Haskell. It is true of a language like\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 🧾 STEP 9: SAVE AND RELOAD YOUR FINE-TUNED MODEL (FIXED)\n",
    "# =====================================================\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 🔹 Load the fine-tuned model (if Jupyter was restarted)\n",
    "model_path = \"./models/gpt2-finetuned\"  # change this if your fine-tuned model is elsewhere\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "print(\"✅ Model and tokenizer loaded from:\", model_path)\n",
    "\n",
    "# =====================================================\n",
    "# 💾 Save the model and tokenizer\n",
    "# =====================================================\n",
    "save_directory = \"./gpt2-finetuned-model\"\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"✅ Model and tokenizer saved successfully at:\", save_directory)\n",
    "\n",
    "# =====================================================\n",
    "# 🔁 Reload the saved model to confirm\n",
    "# =====================================================\n",
    "print(\"\\n🔁 Reloading the model to confirm...\")\n",
    "\n",
    "reloaded_tokenizer = GPT2Tokenizer.from_pretrained(save_directory)\n",
    "reloaded_model = GPT2LMHeadModel.from_pretrained(save_directory)\n",
    "\n",
    "print(\"✅ Reloaded model and tokenizer successfully!\")\n",
    "\n",
    "# =====================================================\n",
    "# 💬 Generate text from reloaded model\n",
    "# =====================================================\n",
    "prompt = \"Once upon a time in the world of AI,\"\n",
    "inputs = reloaded_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"\\n🤖 Generating text...\")\n",
    "outputs = reloaded_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n✨ Model Output:\\n\")\n",
    "print(reloaded_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be209172-c61d-434c-9027-711dcf117a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Fine-tuned GPT-2 model is ready for chat! Type 'quit' to stop.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🧠 You:  What Are Paleo?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 GPT-2: BELLEVUE, Wash. - An online retailer has removed two women from their listings.\n",
      "\n",
      "Bella Yantalova and Krista Yantalova are not on the list.\n",
      "\n",
      "The women's listing on the online store went down Tuesday, and is now being reviewed by a customer service representative.\n",
      "\n",
      "The women's listing was removed by Amazon.com after an internal review. Yantalova is listed as an author and Yantalova as a business\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🧠 You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Chat ended.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 💬 STEP 10a: INTERACTIVE CHAT WITH FINE-TUNED MODEL\n",
    "# =====================================================\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = \"./gpt2-finetuned-model\"  # or wherever you saved it\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"🤖 Fine-tuned GPT-2 model is ready for chat! Type 'quit' to stop.\\n\")\n",
    "\n",
    "# Simple interactive chat loop\n",
    "while True:\n",
    "    user_input = input(\"🧠 You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"stop\"]:\n",
    "        print(\"👋 Chat ended.\")\n",
    "        break\n",
    "\n",
    "    # Encode input\n",
    "    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate model response\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and display response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response[len(user_input):].strip()  # remove repetition\n",
    "\n",
    "    print(f\"🤖 GPT-2: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57eeb827-b1b8-4a27-b20d-34257ffb8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# 💾 Step 10b: Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n",
    "\n",
    "print(\"✅ Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "531005a1-9126-4aa4-82f3-e685f2fcbc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully and ready for chat!\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Step 11: Load the fine-tuned model\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_path = \"./gpt2-finetuned\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "print(\"✅ Model loaded successfully and ready for chat!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305db7a5-9cd2-4677-9163-03aa29082552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (8.3.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (2.3.4)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Using cached pillow-11.3.0-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (6.33.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (3.1.45)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from streamlit) (6.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-2.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\misba\\onedrive\\desktop\\lm_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Using cached streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Using cached pillow-11.3.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading narwhals-2.9.0-py3-none-any.whl (422 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, pillow, narwhals, cachetools, blinker, pydeck, altair, streamlit\n",
      "\n",
      "   -------- -------------------------------  2/10 [tenacity]\n",
      "  Attempting uninstall: pillow\n",
      "   -------- -------------------------------  2/10 [tenacity]\n",
      "    Found existing installation: pillow 12.0.0\n",
      "   -------- -------------------------------  2/10 [tenacity]\n",
      "    Uninstalling pillow-12.0.0:\n",
      "   -------- -------------------------------  2/10 [tenacity]\n",
      "      Successfully uninstalled pillow-12.0.0\n",
      "   -------- -------------------------------  2/10 [tenacity]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ------------ ---------------------------  3/10 [pillow]\n",
      "   ---------------- -----------------------  4/10 [narwhals]\n",
      "   ---------------- -----------------------  4/10 [narwhals]\n",
      "   ---------------- -----------------------  4/10 [narwhals]\n",
      "   ---------------- -----------------------  4/10 [narwhals]\n",
      "   ---------------------------- -----------  7/10 [pydeck]\n",
      "   -------------------------------- -------  8/10 [altair]\n",
      "   -------------------------------- -------  8/10 [altair]\n",
      "   -------------------------------- -------  8/10 [altair]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ------------------------------------ ---  9/10 [streamlit]\n",
      "   ---------------------------------------- 10/10 [streamlit]\n",
      "\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-6.2.1 narwhals-2.9.0 pillow-11.3.0 pydeck-0.9.1 streamlit-1.50.0 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\misba\\OneDrive\\Desktop\\LM_Project\\.venv\\Lib\\site-packages\\~il'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "799b7a4c-998c-4187-89d9-afa973eab50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 21:24:44.595 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.683 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\misba\\OneDrive\\Desktop\\LM_Project\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-24 21:24:44.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.689 Session state does not function when running a script without `streamlit run`\n",
      "2025-10-24 21:24:44.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-24 21:24:44.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# STEP 13: STREAMLIT CHATBOT FOR FINE-TUNED GPT-2\n",
    "# =====================================================\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# -------------------------------\n",
    "# Load fine-tuned model\n",
    "# -------------------------------\n",
    "model_path = \"./gpt2-finetuned\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Make sure model runs on GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Streamlit UI\n",
    "# -------------------------------\n",
    "st.title(\"🤖 GPT-2 Chatbot\")\n",
    "st.write(\"Chat with your fine-tuned GPT-2 model!\")\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"You:\", \"\")\n",
    "\n",
    "if user_input:\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=150, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Display response\n",
    "    st.text_area(\"GPT-2:\", value=response, height=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98c88b-b33e-40f0-a4b6-d1b0ebbd5d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
